{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAGIC %run ../../global_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAGIC %run ../../utils/dashboard_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAGIC %run ../../utils/nlm_eutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAGIC %run ../../utils/solr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.widgets.removeAll()\n",
    "dbutils.widgets.text('airtable_file','')\n",
    "dbutils.widgets.text('airtable_table','')\n",
    "dbutils.widgets.text('prefix','')\n",
    "dbutils.widgets.dropdown('pm_interface', \"eutils\", ['eutils','website'])\n",
    "dbutils.widgets.dropdown('pm_order', \"best_match\", ['best_match','date'])\n",
    "dbutils.widgets.dropdown('delete_database', \"True\", ['True','False'])\n",
    "dbutils.widgets.dropdown('pm_include', \"True\", ['True','False'])\n",
    "dbutils.widgets.dropdown('solr_include', \"False\", ['True','False'])\n",
    "dbutils.widgets.dropdown('epmc_include', \"False\", ['True','False'])\n",
    "dbutils.widgets.text('airtable_subsets_table','')\n",
    "dbutils.widgets.text('airtable_subsets_column','Query')\n",
    "\n",
    "airtable_file = dbutils.widgets.get('airtable_file')\n",
    "airtable_table = dbutils.widgets.get('airtable_table')\n",
    "\n",
    "prefix = dbutils.widgets.get('prefix')\n",
    "\n",
    "pm_interface = dbutils.widgets.get('pm_interface')\n",
    "pm_order = dbutils.widgets.get('pm_order')\n",
    "\n",
    "pm_include = dbutils.widgets.get('pm_include')\n",
    "solr_include = dbutils.widgets.get('solr_include')\n",
    "epmc_include = dbutils.widgets.get('epmc_include')\n",
    "\n",
    "# Any additional conditions for creating this corpus?\n",
    "airtable_subsets_table = dbutils.widgets.get('airtable_subsets_table')\n",
    "delete_database = dbutils.widgets.get('delete_database')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcdash = RstContentDashboard(g_database, g_schema, g_loc, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airtable_api_url = 'https://api.airtable.com/v0/%s/%s?api_key=%s'%(airtable_file, airtable_table, g_airtable_api_key) \n",
    "df = read_airtable(airtable_api_url)\n",
    "displayHTML(df.to_html())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsets_df = None\n",
    "if len(airtable_subsets_table)>0: \n",
    "  airtable_subsets_api_url = 'https://api.airtable.com/v0/%s/%s?api_key=%s'%(airtable_file, airtable_subsets_table, g_airtable_api_key) \n",
    "  subsets_df = read_airtable(airtable_subsets_api_url)\n",
    "  subsets_df = subsets_df.fillna('')\n",
    "  subsets_df.reset_index(inplace=True, drop=True)\n",
    "  displayHTML(subsets_df.to_html())\n",
    "else: \n",
    "  subsets_df = pd.DataFrame([{\"ID\":0, \"Subset_Name\":\"None\", \"Query\":\"\"}])\n",
    "no_subsets_df = pd.DataFrame([{\"ID\":0, \"Subset_Name\":\"None\", \"Query\":\"\"}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE PYEDA TO PROCESS AND REPURPOSE QUERIES AS LOGICAL EXPRESSIONS FOR SEARCHING.\n",
    "import re\n",
    "import pprint\n",
    "from pyeda.inter import *\n",
    "from pyeda.boolalg.expr import Literal,AndOp,OrOp\n",
    "from enum import Enum\n",
    "import unicodedata\n",
    "\n",
    "class QueryType(Enum):\n",
    "  open = 1\n",
    "  closed = 2\n",
    "  solr = 3\n",
    "  epmc = 4\n",
    "  pubmed = 5\n",
    "  andPlusOrPipe = 6\n",
    "  pubmed_no_types = 7\n",
    "\n",
    "class QueryTranslator(): \n",
    "  def __init__(self, df, query_col):\n",
    "    pp = pprint.PrettyPrinter(indent=4)\n",
    "    def fix_errors(expr_string):\n",
    "      q = re.sub('\\s+(AND)\\s+',' & ',expr_string)\n",
    "      q = re.sub('\\s+(OR)\\s+',' | ',q)\n",
    "      q = re.sub('[\\\"\\n]','',q)\n",
    "      q = re.sub('\\[(ti|ab|ft|tiab)\\]',r'_\\g<1>', q).strip()\n",
    "      return q\n",
    "\n",
    "    self.id2terms = {}\n",
    "    self.terms2id = {}\n",
    "    for tt in df[query_col]:\n",
    "      redq = fix_errors(tt.strip())\n",
    "      for t in re.split('[\\&\\|\\(\\)]', redq):\n",
    "        t = re.sub('[\\(\\)]','', t).strip()\n",
    "        #t = re.sub('\\[(ti|ab|ft|tiab)\\]',r'\\g<1>', t).strip()\n",
    "        if len(t)==0:\n",
    "          continue\n",
    "        if self.terms2id.get(t) is None:\n",
    "          id = 't'+str(len(self.terms2id))\n",
    "          self.id2terms[id] = unicodedata.normalize('NFKD', t).encode('ascii', 'ignore').decode('ascii') # convert to ascii for searches via API \n",
    "          self.terms2id[t] = id\n",
    "\n",
    "    ordered_names = sorted(self.terms2id.keys(), key=len, reverse=True)\n",
    "    self.redq_list = []\n",
    "    for row in df.iterrows():\n",
    "      tt = row[1][query_col]\n",
    "      row_id = row[1]['ID']\n",
    "      redq = fix_errors(tt.strip())\n",
    "      for t in ordered_names:\n",
    "        id = self.terms2id[t]\n",
    "        redq = re.sub('\\\\b'+t+'\\\\b', id, redq)\n",
    "      self.redq_list.append((row_id, redq))\n",
    "\n",
    "  def generate_queries(self, query_type:QueryType):\n",
    "    queries = []\n",
    "    IDs = []\n",
    "    for ID, t in tqdm(self.redq_list):\n",
    "      if t:\n",
    "        print(t)\n",
    "        ex = expr(t)\n",
    "        queries.append(self._expand_expr(ex, query_type))\n",
    "      else: \n",
    "        queries.append('')\n",
    "      IDs.append(ID)\n",
    "    return (IDs, queries)\n",
    "    \n",
    "  def _expand_expr(self, ex, query_type:QueryType):\n",
    "    if query_type == QueryType.open:\n",
    "      return self._simple(ex)\n",
    "    elif query_type == QueryType.closed:\n",
    "      return self._closed_quote(ex)\n",
    "    elif query_type == QueryType.solr:\n",
    "      return self._solr(ex)\n",
    "    elif query_type == QueryType.epmc:\n",
    "      return self._epmc(ex)\n",
    "    elif query_type == QueryType.pubmed:\n",
    "      return self._pubmed(ex)\n",
    "    elif query_type == QueryType.andPlusOrPipe:\n",
    "      return self._plusPipe(ex)\n",
    "    elif query_type == QueryType.pubmed_no_types:\n",
    "      return self._pubmed_no_types(ex)\n",
    "\n",
    "  # expand the query as is with AND/OR linkagage, no extension. \n",
    "  # drop search fields\n",
    "  def _simple(self, ex):\n",
    "    if isinstance(ex, Literal):\n",
    "      term = re.sub('_(ti|ab|ft|tiab)', '', self.id2terms[ex.name])\n",
    "      return term\n",
    "    elif isinstance(ex, AndOp):\n",
    "      return '('+' AND '.join([self._simple(x) for x in ex.xs])+')'\n",
    "    elif isinstance(ex, OrOp):\n",
    "      return '('+' OR '.join([self._simple(x) for x in ex.xs])+')'\n",
    "\n",
    "  def _closed_quote(self, ex):\n",
    "    if isinstance(ex, Literal):\n",
    "      term = re.sub('_(ti|ab|ft|tiab)', '', self.id2terms[ex.name])\n",
    "      return '\"'+term+'\"'\n",
    "    elif isinstance(ex, AndOp):\n",
    "      return '('+' AND '.join([self._closed_quote(x) for x in ex.xs])+')'\n",
    "    elif isinstance(ex, OrOp):\n",
    "      return '('+' OR '.join([self._closed_quote(x) for x in ex.xs])+')'\n",
    "  \n",
    "  def _solr(self, ex):\n",
    "    if isinstance(ex, Literal):\n",
    "      p = re.compile('^(.*)_(ti|ab|ft|tiab)')\n",
    "      m = p.match( self.id2terms[ex.name] )\n",
    "      if m:\n",
    "        t = m.group(1)\n",
    "        f = m.group(2)\n",
    "        if f == 'ti':\n",
    "          return '(paper_title:\"%s\")'%(t)\n",
    "        elif f == 'ab':\n",
    "          return '(paper_abstract:\"%s\")'%(t)\n",
    "        elif f == 'tiab':\n",
    "          return '(paper_title:\"%s\" OR paper_abstract:\"%s\")'%(t,t)\n",
    "        elif f == 'ft':\n",
    "          return '(paper_title:\"%s\" OR paper_abstract:\"%s\")'%(t,t)\n",
    "        else :\n",
    "          raise Exception(\"Incorrect field specification, must be 'ti', 'ab', 'tiab', or 'ft': \" + self.id2terms[ex.name] )\n",
    "      else:              \n",
    "        t = self.id2terms[ex.name]\n",
    "        return '(paper_title:\"%s\" OR paper_abstract:\"%s\")'%(t,t)\n",
    "    elif isinstance(ex, AndOp):\n",
    "      return '('+' AND '.join([self._solr(x) for x in ex.xs])+')'\n",
    "    elif isinstance(ex, OrOp):\n",
    "      return '('+' OR '.join([self._solr(x) for x in ex.xs])+')'\n",
    "\n",
    "  def _epmc(self, ex):\n",
    "    if isinstance(ex, Literal):\n",
    "      p = re.compile('^(.*)_(ti|ab|ft|tiab)')\n",
    "      m = p.match( self.id2terms[ex.name] )\n",
    "      if m:\n",
    "        t = m.group(1)\n",
    "        f = m.group(2)\n",
    "        if f == 'ti':\n",
    "          return '(TITLE:\"%s\")'%(t)\n",
    "        elif f == 'ab':\n",
    "          return '(ABSTRACT:\"%s\")'%(t)\n",
    "        elif f == 'tiab':\n",
    "          return '(TITLE:\"%s\" OR ABSTRACT:\"%s\")'%(t,t)\n",
    "        elif f == 'ft':\n",
    "          return '\"%s\"'%(t)\n",
    "        else:\n",
    "          raise Exception(\"Incorrect field specification, must be 'ti', 'ab', 'tiab', or 'ft': \" + self.id2terms[ex.name] )\n",
    "      else:              \n",
    "        t = self.id2terms[ex.name]\n",
    "        return '(paper_title:\"%s\" OR ABSTRACT:\"%s\")'%(t,t)\n",
    "    elif isinstance(ex, AndOp):\n",
    "      return '('+' AND '.join([self._epmc(x) for x in ex.xs])+')'\n",
    "    elif isinstance(ex, OrOp):\n",
    "      return '('+' OR '.join([self._epmc(x) for x in ex.xs])+')'\n",
    "\n",
    "  def _pubmed(self, ex):\n",
    "    if isinstance(ex, Literal):\n",
    "      p = re.compile('^(.*)_(ti|ab|ft|tiab)$')\n",
    "      m = p.match( self.id2terms[ex.name] )\n",
    "      #print(m)\n",
    "      if m:\n",
    "        t = m.group(1)\n",
    "        f = m.group(2)\n",
    "        if f == 'ti':\n",
    "          return '(\"%s\"[ti])'%(t)\n",
    "        elif f == 'ab':\n",
    "          return '(\"%s\"[ab])'%(t)\n",
    "        elif f == 'tiab':\n",
    "          return '(\"%s\"[tiab])'%(t)\n",
    "        elif f == 'ft':\n",
    "          raise Exception(\"Can't run full text query on pubmed currently: \" + self.id2terms[ex.name] )\n",
    "        else:\n",
    "          raise Exception(\"Incorrect field specification, must be 'ti', 'ab', 'tiab', or 'ft': \" + self.id2terms[ex.name] )\n",
    "      else:              \n",
    "        t = self.id2terms[ex.name]\n",
    "        return '()\"%s\"[tiab])'%(t,t)\n",
    "    elif isinstance(ex, AndOp):\n",
    "      return '('+' AND '.join([self._pubmed(x) for x in ex.xs])+')'\n",
    "    elif isinstance(ex, OrOp):\n",
    "      return '('+' OR '.join([self._pubmed(x) for x in ex.xs])+')'\n",
    "    \n",
    "  def _plusPipe(self, ex):\n",
    "    if isinstance(ex, Literal):\n",
    "      return '\"%s\"'%(self.id2terms[ex.name]) \n",
    "    elif isinstance(ex, AndOp):\n",
    "      return '('+'+'.join([self._pubmed(x) for x in ex.xs])+')'\n",
    "    elif isinstance(ex, OrOp):\n",
    "      return '('+'|'.join([self._pubmed(x) for x in ex.xs])+')'\n",
    "\n",
    "qt = QueryTranslator(df, 'TERMS')\n",
    "qt2 = QueryTranslator(subsets_df, 'Query')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Pubmed Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qt.id2terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(corpus_ids, pubmed_queries) = qt.generate_queries(QueryType.pubmed)\n",
    "(subset_ids, pubmed_subset_queries) = qt2.generate_queries(QueryType.pubmed)\n",
    "pubmed_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbt_list = []\n",
    "if pm_include == 'True':\n",
    "  if pm_interface == 'eutils':\n",
    "    for (i, q) in enumerate(pubmed_queries):\n",
    "      for (j, sq) in zip(subset_ids, ['']):\n",
    "        if len(sq)>0:\n",
    "          q = '(%s) AND (%s)'%(q,sq) \n",
    "        q = re.sub('\\s+','+',q)\n",
    "        esq = ESearchQuery(g_pubmed_api_key)\n",
    "        pubmed_pmids = esq.execute_query(q)\n",
    "        print(len(pubmed_pmids))\n",
    "        for pmid in pubmed_pmids:\n",
    "          sbt_list.append((pmid, i, 'eutils', j))\n",
    "          #print(pmid)\n",
    "  else:\n",
    "    for (i, q) in enumerate(pubmed_queries):\n",
    "      for (j, sq) in zip(subset_ids, ['']):\n",
    "        query = 'https://pubmed.ncbi.nlm.nih.gov/?format=pmid&size=10&term='+re.sub('\\s+','+',q)\n",
    "        if pm_order == 'date':\n",
    "          query += '&sort=date'\n",
    "        #print(query)\n",
    "        #query = quote_plus(query)\n",
    "        if len(sq)>0:\n",
    "          query = '(%s) AND (%s)'%(sq) \n",
    "        response = urlopen(query)\n",
    "        data = response.read().decode('utf-8')\n",
    "        soup = BeautifulSoup(data, \"lxml-xml\")\n",
    "        pmids = re.split('\\s+', soup.find('body').text.strip())\n",
    "        for pmid in pmids:\n",
    "          sbt_list.append((int(pmid), i, 'pubmed', 0))\n",
    "else:\n",
    "  print('Skip Pubmed')\n",
    "  sbt_list = []\n",
    "pubmed_df = DataFrame(sbt_list, columns=['ID_PAPER', 'ID_CORPUS', 'SOURCE', 'SUBSET_CODE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## SOLR Queries (less accurate but very fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "BASE_URL = 'http://aps-solr-http.staging.meta-infra.org:80/v1/solr/paper/select'\n",
    "THIS_YEAR = 'AND paper_pub_date: [NOW-12MONTHS TO *]'\n",
    "\n",
    "(corpus_ids, solr_queries) = qt.generate_queries(QueryType.solr)\n",
    "(subset_ids, solr_subset_queries) = qt2.generate_queries(QueryType.solr)\n",
    "subset_ids = [0]\n",
    "solr_subset_queries = ['']\n",
    "\n",
    "def exec_query_with_timeout_and_repeat(url, post_data_hash):\n",
    "  r = requests.post(url, data=post_data_hash, timeout=10)\n",
    "  data = json.loads(r.text)\n",
    "  #print(data)\n",
    "  if data.get('error') is not None:\n",
    "      raise Exception(\"SOLR Error: \" + data.get('error')['msg'])\n",
    "  i = 0\n",
    "  while(data.get('response') is None and i<10):\n",
    "    r = requests.get(url, timeout=10)\n",
    "    data = json.loads(r.text)\n",
    "    i += 1\n",
    "  if data.get('response') is None:\n",
    "    return []\n",
    "  return data\n",
    "\n",
    "def run_solr_query(q, page_size=1000):   \n",
    "  url = BASE_URL + '?wt=python&fl=id&rows=1&start=0&q='+q\n",
    "  print(url)\n",
    "  r = requests.get(url, timeout=10)\n",
    "  #print(r.text)\n",
    "  data = json.loads(r.text)\n",
    "  #print(data)\n",
    "  numFound = data['response']['numFound']\n",
    "  print(q + ', ' + str(numFound) + ' SOLR PAPERS FOUND')\n",
    "  pmids_from_q = set()\n",
    "  for i in tqdm(range(0, numFound, page_size)):\n",
    "      post_data_hash = {\n",
    "          'wt': 'python',\n",
    "          'fl': 'id,doi',\n",
    "          'wt': 'python',\n",
    "          'rows': str(page_size),\n",
    "          'start': i,\n",
    "          'q': '(' + q + ')'\n",
    "      }\n",
    "      #url = BASE_URL + '?wt=python&fl=id&rows='+str(page_size)+'&start='+str(i)+'&q='+q\n",
    "      #r = requests.get(url)\n",
    "      #print(r.text)\n",
    "      data = json.loads(r.text)\n",
    "      for d in data['response']['docs']:\n",
    "        pmids_from_q.add(str(d['id']))\n",
    "      data = exec_query_with_timeout_and_repeat(BASE_URL, post_data_hash)  \n",
    "      for d in data['response']['docs']:\n",
    "        pmids_from_q.add(str(d['id']))\n",
    "      #pp.pprint(data)\n",
    "      #break\n",
    "  return (numFound, list(pmids_from_q))\n",
    "\n",
    "META_API_URL = 'https://api.meta.org/work/'\n",
    "if solr_include == 'True':\n",
    "  dataset_list = []            \n",
    "  sbt_list = []\n",
    "  for (i, q) in enumerate(solr_queries):\n",
    "    for (j, sq) in zip(subset_ids, solr_subset_queries):\n",
    "      if len(sq)>0:\n",
    "        q = '(%s) AND (%s)'%(q,sq) \n",
    "      numFound, solr_ids = run_solr_query(q)\n",
    "      for id in tqdm(solr_ids):\n",
    "        if '-' not in id:\n",
    "          sbt_list.append((id, i, 'solr', j))\n",
    "        elif 'Datafile-' in id: \n",
    "          m = re.search('^Datafile-(\\d+)$', id)\n",
    "          if m:\n",
    "            dfid = m.group(1)\n",
    "            r = requests.get(META_API_URL+'Dataset:'+dfid)\n",
    "            dataset_list.append(json.loads(r.text))\n",
    "      print(\"%d datasets found in KG\"%(len(dataset_list)))\n",
    "      \n",
    "  solr_df = DataFrame(sbt_list, columns=['ID_PAPER', 'ID_CORPUS', 'SOURCE', 'SUBSET_CODE'])\n",
    "  print(\"%d papers found in SOLR\"%(len(sbt_list)))\n",
    "else:\n",
    "  print('Skipping SOLR.')\n",
    "  solr_df = pd.DataFrame()\n",
    "\n",
    "solr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataset_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## European PMC queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(corpus_ids, epmc_queries) = qt.generate_queries(QueryType.epmc)\n",
    "(subset_ids, epmc_subset_queries) = qt2.generate_queries(QueryType.closed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "(corpus_ids, epmc_queries) = qt.generate_queries(QueryType.epmc)\n",
    "(subset_ids, epmc_subset_queries) = qt2.generate_queries(QueryType.closed)\n",
    "\n",
    "def run_empc_query(q, page_size=1000):   \n",
    "  EMPC_API_URL = 'https://www.ebi.ac.uk/europepmc/webservices/rest/search?resultType=idlist&format=JSON&pageSize='+str(page_size)+'&synonym=TRUE'\n",
    "  url = EMPC_API_URL + '&query=' + q\n",
    "  r = requests.get(url, timeout=10)\n",
    "  data = json.loads(r.text)\n",
    "  numFound = data['hitCount']\n",
    "  print(q + ', ' + str(numFound) + ' European PMC PAPERS FOUND')\n",
    "  pmids_from_q = set()\n",
    "  otherIds_from_q = set()\n",
    "  cursorMark = '*'\n",
    "  for i in tqdm(range(0, numFound, page_size)):\n",
    "    url = EMPC_API_URL + '&cursorMark='+cursorMark+'&query=' + q\n",
    "    r = requests.get(url)\n",
    "    data = json.loads(r.text)\n",
    "    #print(data.keys())\n",
    "    if data.get('nextCursorMark'):\n",
    "      cursorMark = data['nextCursorMark']\n",
    "    for d in data['resultList']['result']:\n",
    "      if d.get('pmid'):\n",
    "        pmids_from_q.add(str(d['pmid']))\n",
    "      else: \n",
    "        otherIds_from_q.add(str(d['id']))\n",
    "    #pp.pprint(data)\n",
    "    #break\n",
    "  return (numFound, list(pmids_from_q))\n",
    "\n",
    "if epmc_include == 'True':\n",
    "  epmc_sbt_list = []\n",
    "  for (i, q) in enumerate(epmc_queries):\n",
    "    for (j, sq) in zip(subset_ids, epmc_subset_queries):\n",
    "      query = q\n",
    "      if len(sq)>0:\n",
    "        query = '(%s) AND (%s)'%(q,sq) \n",
    "      numFound, pmid_ids = run_empc_query(query)\n",
    "      for id in tqdm(pmid_ids):\n",
    "        epmc_sbt_list.append((id, i, 'epmc', j))\n",
    "  epmc_df = DataFrame(epmc_sbt_list, columns=['ID_PAPER', 'ID_CORPUS', 'SOURCE', 'SUBSET_CODE'])\n",
    "  print(\"%d papers found in EPMC\"%(len(epmc_sbt_list)))\n",
    "else:\n",
    "  print('Skipping EPMC.')\n",
    "  epmc_df = pd.DataFrame()\n",
    "\n",
    "epmc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## DataCite queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "(corpus_ids, datacite_queries) = qt.generate_queries(QueryType.closed)\n",
    "(subset_ids, datacite_subset_queries) = qt2.generate_queries(QueryType.closed)\n",
    "\n",
    "def run_datacite_query(q, page_size=1000):   \n",
    "  DATACITE_API_URL = 'https://api.datacite.org/dois?'\n",
    "  url = DATACITE_API_URL + 'page[size]=1&query=' + q\n",
    "  r = requests.get(url, timeout=10)\n",
    "  data = json.loads(r.text)\n",
    "  numFound = data.get('meta').get('total')\n",
    "  #print(q + ', ' + str(numFound) + ' Datacite records found ')\n",
    "  print('\\n'+url+'\\n'+str(numFound) + ' Datacite records found ')\n",
    "  df = pd.DataFrame()\n",
    "  for i in tqdm(range(0, numFound, page_size)):\n",
    "    url = DATACITE_API_URL + 'page[size]='+str(page_size)+'&page[number]='+str(i)+'&query=' + q\n",
    "    #print(url)\n",
    "    r = requests.get(url)\n",
    "    data = json.loads(r.text)\n",
    "    df = df.append(pd.DataFrame.from_dict(data['data']))\n",
    "    #for d in data['data']:\n",
    "    #  records.add((d['attributes']['doi'], d['attributes'].get('types',{}).get('resourceType','')))\n",
    "  #print(records)\n",
    "  return (numFound, df)\n",
    "\n",
    "datacite_include = 'True'\n",
    "if datacite_include == 'True':\n",
    "  rdf = pd.DataFrame()\n",
    "  for (i, q) in enumerate(datacite_queries):\n",
    "    for (j, sq) in zip(subset_ids, datacite_subset_queries):\n",
    "      query = q\n",
    "      if len(sq)>0:\n",
    "        query = '(%s) AND (%s)'%(q,sq) \n",
    "      numFound, df = run_datacite_query(query)  \n",
    "      rdf = rdf.append(df)\n",
    "  print(\"%d papers found in DataCite\"%(len(rdf)))\n",
    "else:\n",
    "  print('Skipping DataCite.')\n",
    "  rdf = pd.DataFrame()\n",
    "\n",
    "rdf['resourceType'] = [row.attributes['types'].get('resourceType','') for row in rdf.itertuples()]\n",
    "rdf['title'] = [row.attributes['titles'][0].get('title','') for row in rdf.itertuples()]\n",
    "rdf['description'] = ['\\n'.join([desc['description'] for desc in row.attributes['descriptions']]) for row in rdf.itertuples()]\n",
    "rdf['url'] = [row.attributes.get('url','') for row in rdf.itertuples()]\n",
    "\n",
    "new_rdf = rdf.drop(columns=['attributes','relationships']).reset_index(drop=True)\n",
    "new_rdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## CROSSREF Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "(corpus_ids, crossref_queries) = qt.generate_queries(QueryType.closed)\n",
    "(subset_ids, crossref_subset_queries) = qt2.generate_queries(QueryType.closed)\n",
    "\n",
    "def run_crossref_query(q, page_size=1000):   \n",
    "  CROSSREF_API_URL = 'https://api.crossref.org/works??'\n",
    "  url = CROSSREF_API_URL + 'page[size]=1&query=' + q\n",
    "  r = requests.get(url, timeout=10)\n",
    "  data = json.loads(r.text)\n",
    "  numFound = data.get('meta').get('total')\n",
    "  #print(q + ', ' + str(numFound) + ' Datacite records found ')\n",
    "  print(str(numFound) + ' Datacite records found ')\n",
    "  records = set()\n",
    "  for i in tqdm(range(0, numFound, page_size)):\n",
    "    url = DATACITE_API_URL + 'page[size]='+str(page_size)+'&page[number]='+str(i)+'&query=' + q\n",
    "    r = requests.get(url)\n",
    "    data = json.loads(r.text)\n",
    "    for d in data['data']:\n",
    "      records.add((d['attributes']['doi'], d['attributes'].get('types',{}).get('resourceType','')))\n",
    "  print(records)\n",
    "  return (numFound, list(records))\n",
    "\n",
    "datacite_include = 'True'\n",
    "if datacite_include == 'True':\n",
    "  rlist = []\n",
    "  for (i, q) in enumerate(datacite_queries):\n",
    "    for (j, sq) in zip(subset_ids, datacite_subset_queries):\n",
    "      query = q\n",
    "      if len(sq)>0:\n",
    "        query = '(%s) AND (%s)'%(q,sq) \n",
    "      numFound, records = run_datacite_query(query)  \n",
    "      rlist.extend(records)\n",
    "  print(rlist)\n",
    "  rdf = DataFrame(rlist, columns=['DOI', 'SCHEMA.ORG'])\n",
    "  print(\"%d papers found in DataCite\"%(len(rdf)))\n",
    "else:\n",
    "  print('Skipping DataCite.')\n",
    "  rdf = pd.DataFrame()\n",
    "\n",
    "rdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Add additional data to the landscape analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbt_df = pubmed_df\n",
    "sbt_df = sbt_df.append(solr_df)\n",
    "sbt_df = sbt_df.append(epmc_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dashboard Table Creation within SNOWFLAKE. \n",
    "\n",
    "This section performs several subtasks: \n",
    "1. Set up the RstContentDashboard instance\n",
    "1. Drop any previous instances of the tables in SNOWFLAKE - NOTE WE ALWAYS REBUILD THE WHOL\n",
    "1. Load data from Airtable to describe corpora to be mapped\n",
    "1. Rerun queries to build a subset of derived tables in SNOWFLAKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if delete_database == 'True':\n",
    "  mcdash.drop_database(get_dash_cursor(sf, mcdash))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = get_dash_cursor(sf, mcdash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HACK TO ADD ANOTHER SUBSET CODE TO THE DATABASE\n",
    "#pubmed_df[pubmed_df.SUBSET_CODE==5]\n",
    "#existing_data_df = execute_query(cs, 'SELECT * FROM RARE_CYCLE1_CORPUS_TO_PAPER;', ['ID_PAPER','ID_CORPUS','SOURCE','SUBSET_CODE'])\n",
    "#data_to_upload = existing_data_df.append(pubmed_df[pubmed_df.SUBSET_CODE==5])\n",
    "#table_name = re.sub('PREFIX_', mcdash.prefix, 'PREFIX_CORPUS_TO_PAPER')\n",
    "#data_to_upload.to_csv(g_loc + '/' + table_name + '.tsv', index=False, header=False, sep='\\t')\n",
    "#print(g_loc + '/' + table_name + '.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs.execute(\"BEGIN\")\n",
    "mcdash.upload_wb(cs, df, 'CORPUS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = re.sub('PREFIX_', mcdash.prefix, 'PREFIX_CORPUS_TO_PAPER')\n",
    "sbt_df.to_csv(g_loc + table_name + '.tsv', index=False, header=False, sep='\\t')\n",
    "print(g_loc + table_name + '.tsv')\n",
    "cs.execute('DROP TABLE IF EXISTS ' + table_name + ';')\n",
    "cs.execute('CREATE TABLE ' + table_name + ' IF NOT EXISTS (ID_PAPER INT,ID_CORPUS INT,SOURCE TEXT, SUBSET_CODE INT);')\n",
    "cs.execute('put file://' + g_loc + '/' + table_name + '.tsv' + ' @%' + table_name + ';')\n",
    "cs.execute(\n",
    "    \"copy into \" + table_name + \" from @%\" + table_name + \" FILE_FORMAT=(TYPE=CSV FIELD_DELIMITER=\\'\\\\t\\')\"\n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = get_dash_cursor(sf, mcdash)\n",
    "cs.execute(\"BEGIN\")\n",
    "if subsets_df is not None:\n",
    "  table_name = re.sub('PREFIX_', mcdash.prefix, 'PREFIX_CORPUS_TO_PAPER')\n",
    "  cs.execute('DROP TABLE IF EXISTS ' + table_name + ';')\n",
    "  mcdash.upload_wb(cs, subsets_df, 'SUBSETS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPLOAD PREFIX_CORPUS_TO_PAPER TO SNOWFLAKE\n",
    "table_name = re.sub('PREFIX_', mcdash.prefix, 'PREFIX_CORPUS_TO_PAPER')\n",
    "sbt_df.to_csv(g_loc + table_name + '.tsv', index=False, header=False, sep='\\t')\n",
    "cs.execute('CREATE TABLE ' + table_name + ' IF NOT EXISTS (ID_PAPER INT,ID_CORPUS INT,SOURCE TEXT, SUBSET_CODE INT);')\n",
    "cs.execute('put file://' + g_loc + '/' + table_name + '.tsv' + ' @%' + table_name + ';')\n",
    "cs.execute(\n",
    "    \"copy into \" + table_name + \" from @%\" + table_name + \" FILE_FORMAT=(TYPE=CSV FIELD_DELIMITER=\\'\\\\t\\')\"\n",
    ")  \n",
    "mcdash.build_core_tables_from_pmids(cs)\n",
    "cs.execute(\"COMMIT\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
